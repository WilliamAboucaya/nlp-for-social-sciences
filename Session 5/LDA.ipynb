{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LDA to perform topic modeling\n",
    "\n",
    "In this lab, we are going to extract topics from a French online citizen consultation called *République Numérique*. This consultation, held in 2015, aimed at enriching, criticizing and extending the *République Numérique* law bill in 2015 before it got adopted by the French parliament in 2016.\n",
    "\n",
    "All the works presented here are based on the following paper:\n",
    "\n",
    "> William Aboucaya, Sonia Guehis, Rafael Angarita. Building Online Public Consultation Knowledge Graphs. Text2KG 2023: International Workshop on Knowledge Graph Generation from Text, Co-located with the ESWC 2023, May 2023, Hersonissos, Greece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dependencies\n",
    "\n",
    "First, we are going to import all the dependencies that we will need for this lab. If you cannot run the following code cell, do not forget to [create an environment](https://www.freecodecamp.org/news/how-to-setup-virtual-environments-in-python/), to install the dependencies inside of it (using the command `pip install -r requirements.txt`) and to use it as your Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"./projet-de-loi-numerique-consultation-anonyme.csv\"):\n",
    "    with open(\"./projet-de-loi-numerique-consultation-anonyme.csv\", \"wb\") as f:\n",
    "        dataset_URL = \"https://www.data.gouv.fr/fr/datasets/r/891bca8a-d9c1-4250-bfb2-3d13bf595813\"\n",
    "        r = requests.get(dataset_URL, allow_redirects=True)\n",
    "        f.write(r.content)\n",
    "\n",
    "    print(\"Downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset using `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultation = pd.read_csv(\"./projet-de-loi-numerique-consultation-anonyme.csv\",\n",
    "                               parse_dates=[\"Création\", \"Modification\"], index_col=0,\n",
    "                               dtype={\"Identifiant\": str, \"Titre\": str, \"Lié.à..\": str, \"Contenu\": str, \"Lien\": str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the dataset\n",
    "\n",
    "Now that our dataset is loaded as a `pandas Dataframe`, we are going to clean it by filling some empty cells, removing a formatting issue in the content of our proposals and creating a column aggregating all the content we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultation[\"Lié.à..\"] = consultation[\"Lié.à..\"].fillna(\"Unknown\")\n",
    "consultation[\"Type.de.profil\"] = consultation[\"Type.de.profil\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals = consultation.loc[consultation[\"Type.de.contenu\"] == \"Proposition\"].reset_index().copy()\n",
    "proposals[\"Contenu\"] = proposals[\"Contenu\"].apply(lambda proposal: re.sub(\"Éléments de contexte\\r?\\nExplication de l'article :\\r?\\n\", \"\", re.sub(\"(\\r?\\n)+\", \"\\n\", proposal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals[\"full_contribution\"] = proposals[[\"Titre\", \"Contenu\"]].agg(\". \\n\\n\".join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the proposals\n",
    "\n",
    "Here, we will transform our proposals into numeric vectors. For each term, or n-gram, in a given document, the associated score represents the raw frequency of our term in the document. \n",
    "\n",
    "The objective is to give more weight to the most common terms of our corpus. However, some of the most common words in any text cannot be representative of any topic (e.g., \"the\", \"a\", etc.). These stopwords are filtered out using an existing list available online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(proposals.index)\n",
    "\n",
    "french_stopwords = requests.get(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt\").text.splitlines()\n",
    "tf_vectorizer = CountVectorizer(min_df=2, max_features=1000, stop_words=french_stopwords)\n",
    "tf = tf_vectorizer.fit_transform(proposals[\"full_contribution\"])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a document vector! Or more specifically, let's look at the 50 first features of a document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[0,:50].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values correspond to the number of occurrences of the words of our corpus in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_feature_names[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing LDA\n",
    "\n",
    "Here, we will perform LDA and plot the results for 5, 7 and 10 expected topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words):\n",
    "    row_size = 5\n",
    "\n",
    "    fig, axes = plt.subplots(math.ceil(len(model.components_) / row_size), row_size, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        \n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 5 topics\n",
    "lda_5_topics = LatentDirichletAllocation(\n",
    "    n_components=5,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "doc_topic_5_topics = lda_5_topics.fit_transform(tf)\n",
    "    \n",
    "plot_top_words(lda_5_topics, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 7 topics\n",
    "lda_7_topics = LatentDirichletAllocation(\n",
    "    n_components=7,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "doc_topic_7_topics = lda_7_topics.fit_transform(tf)\n",
    "    \n",
    "plot_top_words(lda_7_topics, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 5 topics\n",
    "lda_10_topics = LatentDirichletAllocation(\n",
    "    n_components=10,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "doc_topic_10_topics = lda_10_topics.fit_transform(tf)\n",
    "    \n",
    "plot_top_words(lda_10_topics, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the results for a given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_idx = 111\n",
    "\n",
    "topic_scores = doc_topic_5_topics[proposal_idx]\n",
    "print(\"Proposal:\")\n",
    "print(proposals.loc[proposal_idx, \"full_contribution\"])\n",
    "print(\"\\n\")\n",
    "print(\"Topic scores:\")\n",
    "for i in range(len(topic_scores)):\n",
    "    print(f\"\\tTopic {i + 1}: {topic_scores[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
