{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERTopic to perform topic modeling\n",
    "\n",
    "In this lab, we are going to extract topics from a French online citizen consultation called *République Numérique*. This consultation, held in 2015, aimed at enriching, criticizing and extending the *République Numérique* law bill in 2015 before it got adopted by the French parliament in 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dependencies\n",
    "\n",
    "First, we are going to import all the dependencies that we will need for this lab. If you cannot run the following code cell, do not forget to [create an environment](https://www.freecodecamp.org/news/how-to-setup-virtual-environments-in-python/), to install the dependencies inside of it (using the command `pip install -r requirements.txt`) and to use it as your Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"./projet-de-loi-numerique-consultation-anonyme.csv\"):\n",
    "    with open(\"./projet-de-loi-numerique-consultation-anonyme.csv\", \"wb\") as f:\n",
    "        dataset_URL = \"https://www.data.gouv.fr/fr/datasets/r/891bca8a-d9c1-4250-bfb2-3d13bf595813\"\n",
    "        r = requests.get(dataset_URL, allow_redirects=True)\n",
    "        f.write(r.content)\n",
    "\n",
    "    print(\"Downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset using `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultation = pd.read_csv(\"./projet-de-loi-numerique-consultation-anonyme.csv\",\n",
    "                               parse_dates=[\"Création\", \"Modification\"], index_col=0,\n",
    "                               dtype={\"Identifiant\": str, \"Titre\": str, \"Lié.à..\": str, \"Contenu\": str, \"Lien\": str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the dataset\n",
    "\n",
    "Now that our dataset is loaded as a `pandas Dataframe`, we are going to clean it by filling some empty cells, removing a formatting issue in the content of our proposals and creating a column aggregating all the content we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultation[\"Lié.à..\"] = consultation[\"Lié.à..\"].fillna(\"Unknown\")\n",
    "consultation[\"Type.de.profil\"] = consultation[\"Type.de.profil\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals = consultation.loc[consultation[\"Type.de.contenu\"] == \"Proposition\"].copy()\n",
    "proposals[\"Contenu\"] = proposals[\"Contenu\"].apply(lambda proposal: re.sub(\"Éléments de contexte\\r?\\nExplication de l'article :\\r?\\n\", \"\", re.sub(\"(\\r?\\n)+\", \"\\n\", proposal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals[\"full_contribution\"] = proposals[[\"Titre\", \"Contenu\"]].agg(\". \\n\\n\".join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing proposals embeddings\n",
    "\n",
    "Here, we will transform our proposals into embedding vectors using the tokenizer of a language model. The embedding of a text represents its position in a n-dimensions vector space, with a simple premise: texts with similar meanings should have similar vectors (even if they do not share common words). \n",
    "\n",
    "In this example, we will use a model called [paraphrase-multilingual-mpnet-base-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2). This model is a \"sentence transformer\", it is specifically trained to embed sentences or paragraphs into a vector space. Moreover, it is a multilingual model, meaning that the same sentence in different languages should produce almost the same embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, as we are going to perform a computing-intensive task, we must identify the most efficient device available to perform it. We do so, using PyTorch which is the back-end that we will use in this lab. We prioritize NVIDIA GPUs with CUDA installed, then Apple Silicon GPUs, and finally CPUs if none of the above is found.\n",
    "\n",
    "If you need help installing the relevant version of PyTorch: https://pytorch.org/get-started/locally/\n",
    "\n",
    "If you have a NVIDIA GPU but you don't know whether you have CUDA installed or not, type the following command:\n",
    "\n",
    "```bash\n",
    "nvcc --version\n",
    "```\n",
    "\n",
    "If you have it installed, you should see the CUDA version installed on your computer. Otherwise, you should install a PyTorch-compatible version (as listed [here](https://pytorch.org/get-started/locally/), row \"Stable CUDA\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print (\"GPU not found.\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download and prepare the language model that we will use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now encode our proposals into 768-dimensions vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals.full_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_embeddings = sentence_model.encode(proposals.full_contribution.tolist(), show_progress_bar=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the embeddings of a proposal look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading vectorizers for statistical representation\n",
    "\n",
    "Now that we have our embeddings, we prepare two additional vectorizers whose job is to produce a statistical representation of the terms in the documents. We use both a simple counter with a list of stopwords as filter, and a more complex one based on a formula called TF-IDF. For each term, or n-gram, in a given document, the TF-IDF score represents the frequency of our term in the document inversely weighted by its frequency in the whole corpus.\n",
    "\n",
    "The objective of this formula is to give more weight to the terms appearing in only a subpart of our corpus rather than those which are the most common but also the least distinctive of specific topics. For example, in this dataset, the words \"République\" or \"Numérique\" would have a high term frequency but would not be distinctive at all of a category of proposal.\n",
    "\n",
    "*N.B.*: Here, we use a slightly modified version of the TF-IDF formula implemented by the BERTopic library to suit the needs of topic modeling tasks. However, the principles remain similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords = requests.get(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt\").text.splitlines()\n",
    "vectorizer_model = CountVectorizer(max_df=0.80, min_df=0.20, stop_words=french_stopwords, ngram_range = (1, 2))\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "Here, we instantiate a BERTopic model based on the model we used to produce our embeddings and our two vectorizers. We impose a minimum of 10 documents per topic, but this value can be modified depending on two main factors:\n",
    "- The size of our dataset: Here, with only a few hundreds of texts, we cannot increase this value too much. But for datasets of hundreds of thousands of texts, it may not be relevant to capture a topic specific to only 10 texts.\n",
    "- Whether we want to identify broad topics covering a large quantity of documents or more fine-grained ones specific to a small subset of the corpus.\n",
    "\n",
    "*N.B.*: we use the `low_memory=True` parameter here as there is a known bug in BERTopic specific to Apple Silicon chips which can lead to potential crashes of the computer, and using this parameter reduces this risk. Please remove it **only if you know what you are doing**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTopic(verbose=True, min_topic_size=5, ctfidf_model=ctfidf_model, vectorizer_model=vectorizer_model, embedding_model=\"paraphrase-multilingual-mpnet-base-v2\", low_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = model.fit_transform(proposals.full_contribution, proposals_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = model.get_topic_info()\n",
    "print(f\"Number of topics: {len(freq)}\")\n",
    "freq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_barchart(top_n_topics=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs= model.get_document_info(proposals.full_contribution)\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying topics similar to a concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_to_test = \"vote en ligne\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topics, similarity = model.find_topics(concept_to_test, top_n = 3)\n",
    "\n",
    "for t, s in zip(similar_topics, similarity):\n",
    "    print(f\"For topic {t}:\")\n",
    "    print(f\"\\tSimilarity with the concept '{concept_to_test}': {s}\")\n",
    "    defining_concepts = [concept[0] for concept in model.get_topic(t)]\n",
    "    print(f\"\\tMost relevant topics: {', '.join(defining_concepts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Topic Hierarchy\n",
    "\n",
    "As you can see, some topics are very close. One thing that could come to mind is how can I reduce the number of topics? The good new is that those topics can be hierarchically organized in order to select the appropriate number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_hierarchy(top_n_topics=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Documents\n",
    "\n",
    "Using the previous method, we can visualize the topics and get insight into their relationships. However, you might want a more fine-grained approach where we can visualize the documents inside the topics to see if they were assigned correctly or whether they make sense. To do so, we can use the `topic_model.visualize_documents()` function. This function recalculates the document embeddings and reduces them to 2-dimensional space for easier visualization purposes.\n",
    "\n",
    "*N.B.*: This process can be very expensive, especially if you want to be able to interact with individual documents (`hide_document_hover=False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_documents(proposals.full_contribution.to_list(), hide_document_hover=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
