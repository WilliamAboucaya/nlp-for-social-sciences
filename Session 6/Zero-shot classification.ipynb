{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot text classification\n",
    "\n",
    "In this lab, we are going perform zero-shot text classification using a NLI model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dependencies\n",
    "\n",
    "First, we are going to import all the dependencies that we will need for this lab. If you cannot run the following code cell, do not forget to [create an environment](https://www.freecodecamp.org/news/how-to-setup-virtual-environments-in-python/), to install the dependencies inside of it (using the command `pip install -r requirements.txt`) and to use it as your Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['HF_HOME'] = os.getcwd() + \"/cache/\"\n",
    "import ssl\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the best device to run the model\n",
    "\n",
    "Since we are going to perform a computing-intensive task, we must identify the most efficient device available to perform it. We do so using PyTorch, which is the back-end that we will use in this lab. We prioritize NVIDIA GPUs with CUDA installed, then Apple Silicon GPUs, and finally CPUs if none of the above is found.\n",
    "\n",
    "If you need help installing the relevant version of PyTorch: https://pytorch.org/get-started/locally/\n",
    "\n",
    "If you have a NVIDIA GPU but you don't know whether you have CUDA installed or not, type the following command:\n",
    "\n",
    "```bash\n",
    "nvcc --version\n",
    "```\n",
    "\n",
    "If you have it installed, you should see the CUDA version installed on your computer. Otherwise, you should install a PyTorch-compatible version (as listed [here](https://pytorch.org/get-started/locally/), row \"Stable CUDA\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download and prepare the language model that we will use! This model is a fine-tuning of a multilingual BGE model on text labelling and NLI datasets.\n",
    "\n",
    "Note that here, we will not load the model directly but use a `pipeline`, which is an interface between you and your model to help you perform a specific task. Here, we use a pipeline designed to facilitate zero-shot classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MoritzLaurer/bge-m3-zeroshot-v2.0\"\n",
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=model_name, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First experiment: Sentiment analysis\n",
    "\n",
    "As a first test, we are simply going to test the model on basic sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's choose our label candidates and the template that we will use to transform them into NLI hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_candidates = [\"joy\", \"anger\", \"sadness\", \"surprise\", \"love\", \"fear\"]\n",
    "hypothesis_template = \"This text expresses {}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our template, we will replace the `{}` symbols with our different labels to produce our NLI hypotheses. We will then evaluate the entailment of our hypotheses with a text used as the premise to assess the relevance of the different labels to classify the said text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"Dauphine's canteen is remarkably better than the one I had at my previous job!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= zeroshot_classifier(premise, label_candidates, hypothesis_template=hypothesis_template, multi_label=True)\n",
    "\n",
    "print(\"Classification results:\")\n",
    "for label, score in zip(output[\"labels\"], output[\"scores\"]):\n",
    "    print(f\"\\tFor label '{label}': {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the results do not sum up to 1. That is because the score for each label is computed individually, which allows us to perform multi-label classification.\n",
    "\n",
    "To decide which label(s) you can choose to apply to your text, there are two main strategies, each having its pros and cons:\n",
    "- Select the label with the highest score\n",
    "- Select all the labels with a score higher than a given threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second experiment: text labeling at the scale of a dataset.\n",
    "\n",
    "Now that we have performed our first zero-shot classifications, we may want to perform it on a larger dataset. Here, we are going to test our model on a dataset of Reddit comments on climate change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = load_dataset(\"cathw/reddit_climate_comment\", split='train', revision=\"refs/convert/parquet\")\n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset cleaning process\n",
    "\n",
    "As a first attempt to clean our dataset, we are going to select only the proposals containing between 20 and 200 words (or more precisely, tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_count(batch):\n",
    "    token_counts = []\n",
    "    for comment in batch[\"CommentBody\"]:\n",
    "        if comment:\n",
    "            token_counts.append(len(word_tokenize(comment, language='english')))\n",
    "        else:\n",
    "            token_counts.append(0)\n",
    "    return {\"CommentNbTokens\": token_counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = dst.map(get_token_count, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = dst.filter(lambda row: 20 <= row[\"CommentNbTokens\"] <= 200, num_proc=4)\n",
    "dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may guess, 100k+ rows may be a bit much for our little experiment. So, to speed up the process, we are going to select 1000 random lines from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = dst.shuffle(seed=1234).select(range(1000))\n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of the Reddit posts\n",
    "\n",
    "Now that we hava a clean and small dataset, we are going to classify the posts according to a series of labels that we will define. I have created a few labels as a starting point, including a dummy label `environment` which should be true for most of the texts in our dataset, but do not hesitate to add more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"greenhouse gases\",\n",
    "    \"recycling\",\n",
    "    \"air pollution\",\n",
    "    \"water pollution\",\n",
    "    \"individual changes\",\n",
    "    \"systemic changes\",\n",
    "    \"decarbonization\",\n",
    "    \"sustainability\",\n",
    "    \"ecologic transition\",\n",
    "    \"environment\",\n",
    "]\n",
    "hypothesis_template = \"This text is about {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = zeroshot_classifier(dst[\"CommentBody\"], labels, hypothesis_template=hypothesis_template, multi_label=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 123\n",
    "\n",
    "print(\"Comment:\")\n",
    "print(dst[\"CommentBody\"][idx], end=\"\\n\\n\")\n",
    "print(\"Classification results:\")\n",
    "for label, score in zip(outputs[idx][\"labels\"], outputs[idx][\"scores\"]):\n",
    "    print(f\"\\tFor label '{label}': {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing a hard labeling\n",
    "\n",
    "Now that we have our entailment scores, we want to transform them into a list of labels for each text. Note that this step is not mandatory, for certain tasks keeping the scores may be the better decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_labeled = {\n",
    "    \"comment\": dst[\"CommentBody\"]\n",
    "}\n",
    "for label in labels:\n",
    "    label_relevance_list = []\n",
    "    #label_score_list = []\n",
    "    for i in range(len(dst[\"CommentBody\"])):\n",
    "        label_idx = outputs[i][\"labels\"].index(label)\n",
    "        label_is_relevant = outputs[i][\"scores\"][label_idx] > threshold\n",
    "        label_relevance_list.append(label_is_relevant)\n",
    "        #label_score_list.append(outputs[i][\"scores\"][label_idx])\n",
    "\n",
    "    comments_labeled[label] = label_relevance_list\n",
    "    #comments_labeled[f\"{label}_score\"] = label_score_list\n",
    "\n",
    "comments_labeled_df = pd.DataFrame(comments_labeled)\n",
    "comments_labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggregated results:\")\n",
    "for label in labels:\n",
    "    print(f\"\\tNumber of comments with label '{label}': {comments_labeled_df[label].sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
