{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference\n",
    "\n",
    "In this lab, we are going to simply experiment with a natural language inference (NLI) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dependencies\n",
    "\n",
    "First, we are going to import all the dependencies that we will need for this lab. If you cannot run the following code cell, do not forget to [create an environment](https://www.freecodecamp.org/news/how-to-setup-virtual-environments-in-python/), to install the dependencies inside of it (using the command `pip install -r requirements.txt`) and to use it as your Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['HF_HOME'] = os.getcwd() + \"/cache/\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the best device to run the model\n",
    "\n",
    "Since we are going to perform a computing-intensive task, we must identify the most efficient device available to perform it. We do so using PyTorch, which is the back-end that we will use in this lab. We prioritize NVIDIA GPUs with CUDA installed, then Apple Silicon GPUs, and finally CPUs if none of the above is found.\n",
    "\n",
    "If you need help installing the relevant version of PyTorch: https://pytorch.org/get-started/locally/\n",
    "\n",
    "If you have a NVIDIA GPU but you don't know whether you have CUDA installed or not, type the following command:\n",
    "\n",
    "```bash\n",
    "nvcc --version\n",
    "```\n",
    "\n",
    "If you have it installed, you should see the CUDA version installed on your computer. Otherwise, you should install a PyTorch-compatible version (as listed [here](https://pytorch.org/get-started/locally/), row \"Stable CUDA\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download and prepare the language model that we will use! This model is a fine-tuning of RoBERTa<sub>LARGE</sub> on NLI datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nli_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "tokenizer_nli = AutoTokenizer.from_pretrained(model_nli_name)\n",
    "model_nli = AutoModelForSequenceClassification.from_pretrained(model_nli_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the texts for our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"My cat is in the tree\"\n",
    "hypothesis = \"There is at least one cat in the tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer_nli(premise, hypothesis, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "logits = model_nli(encoded_text[\"input_ids\"])[\"logits\"][0]\n",
    "probs = torch.softmax(logits, -1).tolist()\n",
    "dict_predicted_probability = {'entailment': probs[0], 'neutral': probs[1], 'contradiction': probs[2]}\n",
    "\n",
    "dict_predicted_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, we can now perform NLI on texts! Let's do a few tests!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
